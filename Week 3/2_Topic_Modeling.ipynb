{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "Topic modeling is the process of automatically identifying topics present in a text corpus by uncovering patterns in word occurrences in those texts.\n",
    "\n",
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA is a Baesian(probabilistic) algorithm that considers that each document is a finite mixture of a set of topics and each topic is an infinite mixture of a set of topic probabilities (or words appearing in the texts).\n",
    "\n",
    "For example:\n",
    "\n",
    "Document1 = x% Topic1 + y% Topic2 + z% Topic 3\n",
    "\n",
    "Topic1 = a% Word1 + b% Word2 + c% Word3 + ...\n",
    "\n",
    "The number of topics, as said, is finite and is chosen by the programmer while the number of topic probabilities(or words per topic) is the property of the identified topics. As you go on increasing the word limit per topic AFTER training the algorithm, you will be able to see the allocated probabilities of words for a certain topic. As the probabilities of the words go lower and lower, the significance of the word with respect to that particular topic reduces. The topic with the highest probability is called the **dominant topic** for a particular document.\n",
    "\n",
    "**How to determine the optimum number of topics**\n",
    "\n",
    "Using topic cohenerence value, we can determine if the a set of words in the topic support the overall topic cluster or not.\n",
    "\n",
    "Ususally, with increasing number of topics in a set of documents, result in increasing coherence values. The number topics for which the curve is at the highest and then flattens out (similar to elbow method of finding optimum K for K-Means Clustering) should ideally be the number of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:19:36.875605Z",
     "start_time": "2021-09-22T13:19:35.462669Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "import contractions\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:00:16.834403Z",
     "start_time": "2021-09-22T13:00:16.474832Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Datasets/internet_news_data/articles_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:00:16.994668Z",
     "start_time": "2021-09-22T13:00:16.944359Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:00:49.017555Z",
     "start_time": "2021-09-22T13:00:48.986089Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[~df['description'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:00:53.892668Z",
     "start_time": "2021-09-22T13:00:53.845272Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:01:00.668232Z",
     "start_time": "2021-09-22T13:01:00.642997Z"
    }
   },
   "outputs": [],
   "source": [
    "df.description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:55:25.011004Z",
     "start_time": "2021-09-22T14:55:25.001072Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(x):\n",
    "    # Keep only texts and spaces\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\d\\s]+', '', x)\n",
    "    word_list = []\n",
    "    # for each word, expand contractions\n",
    "    for each_word in cleaned_text.split(' '):\n",
    "        word_list.append(contractions.fix(each_word).lower())\n",
    "    # for each word in expanded list, get lemmas if not in STOPWORDS list\n",
    "    word_list = [wnl.lemmatize(each_word.strip()) for each_word in word_list if each_word not in STOPWORDS and each_word.strip() != '' and len(each_word.strip()) > 1]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:55:29.073435Z",
     "start_time": "2021-09-22T14:55:29.036560Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_text(df.description[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:55:34.757457Z",
     "start_time": "2021-09-22T14:55:29.878749Z"
    }
   },
   "outputs": [],
   "source": [
    "df['description_cleaned'] = df['description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dictionary & Corpus for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:55:35.230877Z",
     "start_time": "2021-09-22T14:55:35.219915Z"
    }
   },
   "outputs": [],
   "source": [
    "list_all_descriptions = list(df['description_cleaned']) # Format - List of list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:55:35.708283Z",
     "start_time": "2021-09-22T14:55:35.696324Z"
    }
   },
   "outputs": [],
   "source": [
    "list_all_descriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:55:36.788681Z",
     "start_time": "2021-09-22T14:55:36.220573Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(list_all_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:55:37.518240Z",
     "start_time": "2021-09-22T14:55:37.225219Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(each_description) for each_description in list_all_descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:55:37.931859Z",
     "start_time": "2021-09-22T14:55:37.918904Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[0] # format - (word_id, word_frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:55:38.440163Z",
     "start_time": "2021-09-22T14:55:38.411259Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word[20] # tesla appears twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:56:58.815553Z",
     "start_time": "2021-09-22T14:55:39.266406Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10,\n",
    "                                            random_state=42,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:56:59.291975Z",
     "start_time": "2021-09-22T14:56:59.263066Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:57:11.892852Z",
     "start_time": "2021-09-22T14:56:59.755379Z"
    }
   },
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                     texts=list_all_descriptions,\n",
    "                                     dictionary=id2word,\n",
    "                                     coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: {}'.format(coherence_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the optimum number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:57:12.350325Z",
     "start_time": "2021-09-22T14:57:12.338366Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_optimum_num_topics(min_topics, max_topics, steps=2, graph=True):\n",
    "    list_models = []\n",
    "    list_coherence_score = []\n",
    "    for i in range(min_topics, max_topics + 1, steps):\n",
    "        print(\"Number of Topics: {}\".format(i))\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=i,\n",
    "                                                    random_state=42,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=10,\n",
    "                                                    alpha='auto',\n",
    "                                                    per_word_topics=True)\n",
    "        list_models.append(lda_model)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                             texts=list_all_descriptions,\n",
    "                                             dictionary=id2word,\n",
    "                                             coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        list_coherence_score.append(coherence_lda)\n",
    "    if graph:\n",
    "        sns.lineplot(x=list(range(min_topics, max_topics + 1, steps)), y=list_coherence_score)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:01:39.382305Z",
     "start_time": "2021-09-22T14:57:12.798929Z"
    }
   },
   "outputs": [],
   "source": [
    "get_optimum_num_topics(5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:03:03.399817Z",
     "start_time": "2021-09-22T15:01:39.921509Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=7,\n",
    "                                            random_state=42,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:03:03.924018Z",
     "start_time": "2021-09-22T15:03:03.911064Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:32:26.029665Z",
     "start_time": "2021-09-22T15:32:26.020693Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=list_all_descriptions):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: x[1], reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:33:35.985378Z",
     "start_time": "2021-09-22T15:32:27.177826Z"
    }
   },
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=list_all_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:33:36.365108Z",
     "start_time": "2021-09-22T15:33:36.336209Z"
    }
   },
   "outputs": [],
   "source": [
    "df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:33:36.759790Z",
     "start_time": "2021-09-22T15:33:36.733879Z"
    }
   },
   "outputs": [],
   "source": [
    "df_topic_sents_keywords['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300.792px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
