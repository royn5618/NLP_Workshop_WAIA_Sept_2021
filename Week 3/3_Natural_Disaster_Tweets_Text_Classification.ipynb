{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:56:49.278241Z",
     "start_time": "2021-09-22T13:56:49.244641Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import contractions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.size'] = 15\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:30.322364Z",
     "start_time": "2021-09-22T13:40:30.233385Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../Datasets/disaster_tweet/train.csv')\n",
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:30.375929Z",
     "start_time": "2021-09-22T13:40:30.326925Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "1. Mixed case\n",
    "2. Contractions\n",
    "3. Hashtags and mentions\n",
    "4. Incorrect spellings\n",
    "5. Punctuations\n",
    "6. websites and urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:30.391890Z",
     "start_time": "2021-09-22T13:40:30.379598Z"
    }
   },
   "outputs": [],
   "source": [
    "all_text = ' '.join(list(df_train['text']))\n",
    "\n",
    "def check_texts(check_item, all_text):\n",
    "    return check_item in all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:30.433589Z",
     "start_time": "2021-09-22T13:40:30.398167Z"
    }
   },
   "outputs": [],
   "source": [
    "print(check_texts('<a', all_text))\n",
    "print(check_texts('<div', all_text))\n",
    "print(check_texts('<p', all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:30.464727Z",
     "start_time": "2021-09-22T13:40:30.442255Z"
    }
   },
   "outputs": [],
   "source": [
    "print(check_texts('#x', all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:30.482854Z",
     "start_time": "2021-09-22T13:40:30.464727Z"
    }
   },
   "outputs": [],
   "source": [
    "print(check_texts(':)', all_text))\n",
    "print(check_texts('<3', all_text))\n",
    "print(check_texts('heard', all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:30.517607Z",
     "start_time": "2021-09-22T13:40:30.485769Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    ''' This method takes in text to remove urls and website links, if any'''\n",
    "    url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_html_entities(text):\n",
    "    ''' This method removes html tags'''\n",
    "    html_entities = r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\n",
    "    text = re.sub(html_entities, '', text)\n",
    "    return text\n",
    "\n",
    "def convert_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def detect_news(text):\n",
    "    if 'news' in text:\n",
    "        text = text + ' news'\n",
    "    return text\n",
    "\n",
    "def remove_social_media_tags(text):\n",
    "    ''' This method removes @ and # tags'''\n",
    "    tag_pattern = r'@([a-z0-9]+)|#'\n",
    "    text = re.sub(tag_pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Count it before I remove them altogether\n",
    "def count_punctuations(text):\n",
    "    getpunctuation = re.findall('[.?\"\\'`\\,\\-\\!:;\\(\\)\\[\\]\\\\/“”]+?',text)\n",
    "    return len(getpunctuation)\n",
    "\n",
    "\n",
    "def preprocess_text(x):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\d\\s]+', '', x)\n",
    "    word_list = []\n",
    "    for each_word in cleaned_text.split(' '):\n",
    "        word_list.append(contractions.fix(each_word).lower())\n",
    "    word_list = [wnl.lemmatize(each_word.strip()) for each_word in word_list if each_word not in STOPWORDS and each_word.strip() != '']\n",
    "    return \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:35.600755Z",
     "start_time": "2021-09-22T13:40:30.521882Z"
    }
   },
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(remove_urls)\n",
    "df_train['text'] = df_train['text'].apply(remove_html_entities)\n",
    "df_train['text'] = df_train['text'].apply(convert_lower_case)\n",
    "df_train['text'] = df_train['text'].apply(detect_news)\n",
    "df_train['text'] = df_train['text'].apply(remove_social_media_tags)\n",
    "df_train['punctuation_count'] = df_train['text'].apply(count_punctuations)\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "\n",
    "df_train['text_tokenized'] = df_train['text'].apply(word_tokenize)\n",
    "df_train['words_per_tweet'] = df_train['text_tokenized'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:35.671661Z",
     "start_time": "2021-09-22T13:40:35.600755Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:36.858846Z",
     "start_time": "2021-09-22T13:40:35.671661Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(x='words_per_tweet', hue='target', data=df_train, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:37.804389Z",
     "start_time": "2021-09-22T13:40:36.858846Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='target', hue='punctuation_count', data=df_train)\n",
    "plt.legend([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Text Analysis using WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:37.828120Z",
     "start_time": "2021-09-22T13:40:37.806743Z"
    }
   },
   "outputs": [],
   "source": [
    "real_disaster_tweets = ' '.join(list(df_train[df_train['target'] == 1]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:37.856172Z",
     "start_time": "2021-09-22T13:40:37.828593Z"
    }
   },
   "outputs": [],
   "source": [
    "real_disaster_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:37.883524Z",
     "start_time": "2021-09-22T13:40:37.861441Z"
    }
   },
   "outputs": [],
   "source": [
    "non_real_disaster_tweets = ' '. join(list(df_train[df_train['target'] == 0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:42.282475Z",
     "start_time": "2021-09-22T13:40:37.888667Z"
    }
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"black\", \n",
    "               max_words=100, \n",
    "               width=1000, \n",
    "               height=600, \n",
    "               random_state=1).generate(real_disaster_tweets)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud of Tweets about Real Disasters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:40:46.514960Z",
     "start_time": "2021-09-22T13:40:42.282475Z"
    }
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"black\", \n",
    "               max_words=100, \n",
    "               width=1000, \n",
    "               height=600,\n",
    "               font_step=1,\n",
    "               random_state=1).generate(non_real_disaster_tweets)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud of Tweets NOT about Real Disasters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:43:44.783635Z",
     "start_time": "2021-09-22T13:43:44.765989Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:49:22.554059Z",
     "start_time": "2021-09-22T13:49:22.530261Z"
    }
   },
   "outputs": [],
   "source": [
    "target = df_train['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['text'], target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:49:23.610585Z",
     "start_time": "2021-09-22T13:49:23.599891Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(min_df=0.,\n",
    "                         max_df=1.,\n",
    "                         use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:49:24.909914Z",
     "start_time": "2021-09-22T13:49:24.657890Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vector = tf_idf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:49:25.814773Z",
     "start_time": "2021-09-22T13:49:25.787775Z"
    }
   },
   "outputs": [],
   "source": [
    "list(tfidf_vector.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:49:39.750973Z",
     "start_time": "2021-09-22T13:49:39.516262Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf_train = tf_idf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:49:53.229828Z",
     "start_time": "2021-09-22T13:49:53.135508Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf_test = tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:50:04.331873Z",
     "start_time": "2021-09-22T13:50:04.073016Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(tf_idf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:50:08.729511Z",
     "start_time": "2021-09-22T13:50:08.708607Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_log_reg = clf.predict(tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:50:12.278275Z",
     "start_time": "2021-09-22T13:50:12.253800Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:50:19.310809Z",
     "start_time": "2021-09-22T13:50:19.281548Z"
    }
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_log_reg), recall_score(y_test, y_pred_log_reg), f1_score(y_test, y_pred_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future Work**\n",
    "\n",
    "1. Testing different algorithms\n",
    "2. Test different tf-idf settings, like set idf=False, then, only term frequency will be considered\n",
    "3. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe and Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Layer - learn an embedding for all of the words in the training dataset. It is defined as the first hidden layer of a network.\n",
    "\n",
    "It is a flexible layer that can be used in a variety of ways, such as:\n",
    "* It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "* It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "* It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
    "\n",
    "Resources - \n",
    "\n",
    "1. https://keras.io/api/layers/core_layers/embedding/\n",
    "\n",
    "2. https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras API's Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:10:03.716740Z",
     "start_time": "2021-09-22T14:10:03.450084Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:14:11.172817Z",
     "start_time": "2021-09-22T14:10:06.193257Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_file = r'C:\\Users\\nroy0\\Documents\\Resources\\glove.6B\\glove.6B.300d.txt'\n",
    "w2v_file = get_tmpfile(\"glove_w2v.txt\")\n",
    "glove2word2vec(glove_file, w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:16:04.907798Z",
     "start_time": "2021-09-22T14:14:11.252552Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:16:06.036315Z",
     "start_time": "2021-09-22T14:16:04.988358Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = w2v_model.key_to_index.keys()\n",
    "\n",
    "glove_embedding_matrix = w2v_model[vocab]\n",
    "glove_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:27:06.875558Z",
     "start_time": "2021-09-22T14:27:06.859615Z"
    }
   },
   "outputs": [],
   "source": [
    "target = df_train['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['text'], target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:27:07.351598Z",
     "start_time": "2021-09-22T14:27:07.098763Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 1000\n",
    "tokenizer = Tokenizer(oov_token = \"<OOV>\", num_words=max_length)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "padded_train = pad_sequences(sequences_train, padding = 'post', maxlen=max_length)\n",
    "padded_test = pad_sequences(sequences_test, padding = 'post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:27:07.618889Z",
     "start_time": "2021-09-22T14:27:07.573275Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "keys_not_present = []\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError as e:\n",
    "        keys_not_present.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:27:07.825329Z",
     "start_time": "2021-09-22T14:27:07.810340Z"
    }
   },
   "outputs": [],
   "source": [
    "print(keys_not_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:27:08.031681Z",
     "start_time": "2021-09-22T14:27:08.018759Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:27:08.252662Z",
     "start_time": "2021-09-22T14:27:08.239466Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, \n",
    "                        300, \n",
    "                        weights=[embedding_matrix], \n",
    "                        input_length=max_length,\n",
    "                        trainable=False))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:27:08.598966Z",
     "start_time": "2021-09-22T14:27:08.582076Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                  patience=15,\n",
    "                                  verbose=1,\n",
    "                                  mode=\"min\",\n",
    "                                  restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='model.hdf5',\n",
    "                                    verbose=1,\n",
    "                                    save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:27:08.897925Z",
     "start_time": "2021-09-22T14:27:08.821948Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:27:09.139639Z",
     "start_time": "2021-09-22T14:27:09.124236Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:36:16.349718Z",
     "start_time": "2021-09-22T14:34:25.166493Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True) \n",
    "## Stackoverflow - https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio\n",
    "history = model.fit(padded_train, \n",
    "                    y_train, \n",
    "                    epochs=50, \n",
    "                    validation_data=(padded_test, y_test), \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T14:38:44.333022Z",
     "start_time": "2021-09-22T14:38:43.443202Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model.hdf5')\n",
    "y_pred = (model.predict(padded_test) > 0.5).astype(\"int32\")\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Embedding Layer + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:59:09.858614Z",
     "start_time": "2021-09-22T15:59:09.849637Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, \n",
    "                        300, \n",
    "                        weights=[embedding_matrix], \n",
    "                        input_length=max_length,\n",
    "                        trainable=False))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:59:10.912146Z",
     "start_time": "2021-09-22T15:59:10.899189Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                  verbose=1,\n",
    "                                  mode=\"min\",\n",
    "                                  restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='lstm_model.hdf5',\n",
    "                                    verbose=1,\n",
    "                                    save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:59:12.453300Z",
     "start_time": "2021-09-22T15:59:11.435546Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_lstm_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:59:12.959986Z",
     "start_time": "2021-09-22T15:59:12.916132Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-22T15:59:12.310Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "## Stackoverflow - https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio\n",
    "history = model.fit(padded_train, \n",
    "                    y_train, \n",
    "                    epochs=2, \n",
    "                    validation_data=(padded_test, y_test), \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-22T15:59:17.664Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('lstm_model.hdf5')\n",
    "y_pred = (model.predict(padded_test) > 0.5).astype(\"int32\")\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
