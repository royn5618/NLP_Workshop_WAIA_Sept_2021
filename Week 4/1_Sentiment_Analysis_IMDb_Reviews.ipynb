{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:46:06.220779Z",
     "start_time": "2021-09-28T23:46:06.198280Z"
    },
    "id": "EDjVkn6S9Nfi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:46:07.037517Z",
     "start_time": "2021-09-28T23:46:06.226512Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "czGB-XSF9YhP",
    "outputId": "30d0b8fa-159d-4886-9738-d4fb56ff34f8"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../Datasets/imdb_review/train_data.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:46:07.082091Z",
     "start_time": "2021-09-28T23:46:07.039892Z"
    },
    "id": "NwTNOVm6917_"
   },
   "outputs": [],
   "source": [
    "def test_casing(x):\n",
    "    if x.isupper():\n",
    "        print('Contains Upper Case')\n",
    "\n",
    "\n",
    "_ = df_train.SentimentText.apply(test_casing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7paNWLa2-xOu"
   },
   "source": [
    "**Observations**\n",
    "\n",
    "1. Contains single letters\n",
    "2. All lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:46:07.538184Z",
     "start_time": "2021-09-28T23:46:07.084361Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bgya1HVz-qkm",
    "outputId": "d180b5c0-e7a0-4577-9e11-5b0d693c84d3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:46:07.565464Z",
     "start_time": "2021-09-28T23:46:07.544854Z"
    },
    "id": "ej_U2m3h_eZe"
   },
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    \"\"\"\n",
    "    This method removes stopwords, removes single characters and lemmatizes the words\n",
    "    \"\"\"\n",
    "    return \" \".join([\n",
    "        lemmatizer.lemmatize(each_token.strip()) for each_token in x.split(' ')\n",
    "        if each_token not in STOPWORDS and len(each_token) > 1\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:46:57.740754Z",
     "start_time": "2021-09-28T23:46:07.569275Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "78aEJdSp_CDu",
    "outputId": "27e2e862-261e-4955-82b9-07b7fdf87236"
   },
   "outputs": [],
   "source": [
    "df_train['SentimentTextCleaned'] = df_train.SentimentText.apply(clean_text)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:46:57.754449Z",
     "start_time": "2021-09-28T23:46:57.740754Z"
    },
    "id": "up8f6IRmMVl-"
   },
   "outputs": [],
   "source": [
    "# !pip install gensim==4.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:11.702091Z",
     "start_time": "2021-09-28T23:46:57.756880Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uX6TRaIJddA",
    "outputId": "dd0cb26e-4040-4e78-ca91-eb7b35b2a11e"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)\n",
    "\n",
    "# Prepare the data\n",
    "sentences = [word_tokenize(each_text) for each_text in list(df_train['SentimentTextCleaned'])]\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "To pass the data to Gensim, the format of the input data is -\n",
    "\n",
    "```\n",
    "[ \n",
    "    [ token 1, token 2, token 3, ..... ], # signifies text in one row of a dataframe or one sentence in a document\n",
    "    [ token 2, token 31, token 12, ....], # The token numbers are random\n",
    "    ......\n",
    "    [ token 1, token 16, token 91, .....]\n",
    "]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:48.947037Z",
     "start_time": "2021-09-28T23:47:11.702091Z"
    },
    "id": "DOnhR68XPVTw"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the Word2Vec SkipGram model\n",
    "w2v = Word2Vec(sentences=sentences, \n",
    "               vector_size=100, \n",
    "               window=5,\n",
    "               max_vocab_size=10000,\n",
    "               min_count=2,  \n",
    "               sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:48.971566Z",
     "start_time": "2021-09-28T23:47:48.947037Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iInIxLWKnCt",
    "outputId": "330e4833-0e0b-4886-942a-392a3246f89d"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(w2v.wv)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:49.034919Z",
     "start_time": "2021-09-28T23:47:48.976691Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIvIi-QmKBiw",
    "outputId": "045a57e9-133a-4920-b6f0-171d0ef0763c"
   },
   "outputs": [],
   "source": [
    "vocabulary = list(w2v.wv.key_to_index.keys())\n",
    "vocab_word_vec = w2v.wv[vocabulary]\n",
    "vocab_word_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "```w2v.wv['any word']``` will give me an array of the 100 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:49.052313Z",
     "start_time": "2021-09-28T23:47:49.039975Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3G6Th3b9Yku",
    "outputId": "975e72c6-b154-4100-8554-697246d3f646"
   },
   "outputs": [],
   "source": [
    "vocab_word_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:49.067938Z",
     "start_time": "2021-09-28T23:47:49.052861Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TE4PZumvdeOm",
    "outputId": "9f654599-c800-442e-d889-a606f6823118"
   },
   "outputs": [],
   "source": [
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:49.095628Z",
     "start_time": "2021-09-28T23:47:49.067938Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdECPn56dX1h",
    "outputId": "3a384134-8ef1-483b-9037-782f89bf4e25"
   },
   "outputs": [],
   "source": [
    "w2v.wv.most_similar('cartoon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "\n",
    "Similarity is based on Cosine similarity which is a mathematical technique to measure distance between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:49.112004Z",
     "start_time": "2021-09-28T23:47:49.095628Z"
    },
    "id": "zC0uoTace1ah"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def plot_similarity_PCA(model, word_vector, vocabulary):\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(word_vector)\n",
    "    print(result.shape)\n",
    "    plt.scatter(\n",
    "        result[:, 0], # column (dimesion) 1\n",
    "        result[:, 1], # column (dimension) 2\n",
    "        color='b'\n",
    "    )\n",
    "    # annotation or printing words in the plot\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:50.572998Z",
     "start_time": "2021-09-28T23:47:49.112004Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "oMqvFmx5fBGh",
    "outputId": "89b2c261-228a-4cc5-86d7-d952b635e212"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 10])\n",
    "plot_similarity_PCA(w2v, w2v.wv[vocabulary[:100]], vocabulary[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    " \n",
    "This visualization is for demonstration only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:47:50.602053Z",
     "start_time": "2021-09-28T23:47:50.573659Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hhG1WdTLU-J",
    "outputId": "7f9fd51c-0fbc-4e45-d6d8-63466386b0fb"
   },
   "outputs": [],
   "source": [
    "# Fill back with word index from vocabulary\n",
    "\n",
    "w2v.wv.key_to_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:05.699852Z",
     "start_time": "2021-09-28T23:47:50.602053Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0Wn8WdSaQdfV",
    "outputId": "f4fd54a1-99d5-44a8-d9ae-cc3c3fde5de2"
   },
   "outputs": [],
   "source": [
    "df_train['SentimentTextTokenized'] = df_train['SentimentTextCleaned'].apply(word_tokenize)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:05.714486Z",
     "start_time": "2021-09-28T23:48:05.700867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErA6QmDvRA8t",
    "outputId": "9a1f114d-8755-40d8-a91d-cc230f01d96c"
   },
   "outputs": [],
   "source": [
    "def map_vocab_index(tokenized_review):\n",
    "    \"\"\"\n",
    "    This function maps the word index from the vocabulary and creates a list of the indices\n",
    "    For example: If index of the word 'film' is 1 and 'movie' is 2, \n",
    "    then for a text like ['film', 'movie'], the output of this function will be [1, 2]\n",
    "    \"\"\"\n",
    "    vocab_index_mapped_review = [\n",
    "        w2v.wv.key_to_index[each_token] for each_token in tokenized_review\n",
    "        if each_token in w2v.wv.key_to_index.keys()\n",
    "    ]\n",
    "    return vocab_index_mapped_review\n",
    "\n",
    "\n",
    "# TEST\n",
    "map_vocab_index(['movie', 'film', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:06.976853Z",
     "start_time": "2021-09-28T23:48:05.717028Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9MxlD-1jRmm8",
    "outputId": "9a30e5c5-8394-438e-bcce-e1928f13e686"
   },
   "outputs": [],
   "source": [
    "df_train['SentimentTextVocabIndexed'] = df_train['SentimentTextTokenized'].apply(map_vocab_index)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:07.004684Z",
     "start_time": "2021-09-28T23:48:06.976853Z"
    },
    "id": "e4_42DsjSucW"
   },
   "outputs": [],
   "source": [
    "feature_column = 'SentimentTextVocabIndexed'\n",
    "target_column = 'Sentiment'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[feature_column].values, \n",
    "                                                    df_train[target_column].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardising Input Text\n",
    "\n",
    "We will be using a Deep Learning model for predicting sentiment. \n",
    "\n",
    "Embedding Layer from Keras API is used a the layer 0 (or the first layer) to pass word embeddings into a NN.\n",
    "\n",
    "More information on Embedding Layer -https://keras.io/api/layers/core_layers/embedding/\n",
    "\n",
    "In order to feed texts into an Embedding Layer, each input text (a sequence of tokens) should be of a particular length.\n",
    "\n",
    "Example: \n",
    "1. Sentence 1 - [3, 6, 21, 67, 32]\n",
    "2. Sentence 2 - [1, 2, 6, 12, 34, 45, 7, 8]\n",
    "3. Sentence 3 - [21, 2, 31]\n",
    "\n",
    "These sentences will have to be of a particular length, say 5.\n",
    "\n",
    "Then, on using Padding [pad_sequences from Keras API], the following will be achieved:\n",
    "\n",
    "1. Sentence 1 - [3, 6, 21, 67, 32]\n",
    "2. Sentence 2 - [1, 2, 6, 12, 34]\n",
    "3. Sentence 3 - [21, 2, 31, 0 , 0]\n",
    "\n",
    "You can use ```padding``` and ```truncating``` as pre or post to convey from where the cut-off or padding will be applied.\n",
    "\n",
    "Hence, a length analysis is done here to see what sort of length of reviews are there in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:07.219247Z",
     "start_time": "2021-09-28T23:48:07.004684Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "s5t1Gwy6Rym1",
    "outputId": "ed18ca58-d0bb-42c4-ee7a-056dd8694d2c"
   },
   "outputs": [],
   "source": [
    "# Length Analysis\n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(data=list(df_train['SentimentTextVocabIndexed'].apply(len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx8WPqhCSNV9"
   },
   "source": [
    "**Observation**\n",
    "\n",
    "Most of the reviews are of about 100 to 175 in length. Lert us set the maximum length of a review as 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:07.236093Z",
     "start_time": "2021-09-28T23:48:07.225259Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saWLvYYCg1eP",
    "outputId": "e55ad8ff-744d-44b0-aada-ca751eb6a51b"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100 # as we had set while training the word2vec model\n",
    "max_len = 200  # Length of input - All input should have the same length - if length over 200, the input will be truncated at 200.\n",
    "vocabulary_size, embedding_dim, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:07.631531Z",
     "start_time": "2021-09-28T23:48:07.237971Z"
    },
    "id": "bWklYNYHT-Mu"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_len, truncating='post',padding='post')\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_len, truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "Here we have used an Embedding layer along with Flatten and Dense Layers.\n",
    "\n",
    "In the embedding layer, you specify the vocabulary length, the embedding dimansion and then the length of the input. Additionally, the weights are also initiated with the word's vectors or word's embeddings. This is transfering the learned weights from the Skip Gram model into the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:07.734731Z",
     "start_time": "2021-09-28T23:48:07.631531Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GKR2UG8Suh9",
    "outputId": "15f3b733-25fd-449c-adb3-7ee7280ef5be"
   },
   "outputs": [],
   "source": [
    "# Refer - https://keras.io/api/layers/core_layers/embedding/\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, # 1 + Vocab Size or 1 + max vocab index\n",
    "                    output_dim=embedding_dim, # Dimnsionality of word embeddings\n",
    "                    input_length=max_len,\n",
    "                    weights=[vocab_word_vec],\n",
    "                    trainable=False\n",
    "                    ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) # sigmoid since binary classification\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc']\n",
    "              )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:44.424393Z",
     "start_time": "2021-09-28T23:48:07.734731Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejbHBXglUj6f",
    "outputId": "85868564-ff08-4d61-bc09-b388b3efd6ba"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_padded, \n",
    "          y_train,\n",
    "          epochs=5,\n",
    "          validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "\n",
    "Experiment with different configurations and plot the train/val loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:55.927264Z",
     "start_time": "2021-09-28T23:48:55.115308Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkP9d_fYiGfY",
    "outputId": "306e2c60-4976-4fcf-ebf5-fc2884302ece"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtuwe3UyiAg3"
   },
   "source": [
    "## LSTM Implementation\n",
    "\n",
    "Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:48:59.991128Z",
     "start_time": "2021-09-28T23:48:59.462574Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(\n",
    "    Embedding(\n",
    "        input_dim=vocabulary_size,  # 1 + Vocab Size or 1 + max vocab index\n",
    "        output_dim=embedding_dim,  # Dimnsionality of word embeddings\n",
    "        input_length=max_len,\n",
    "        weights=[vocab_word_vec],\n",
    "        trainable=False))\n",
    "lstm_model.add(LSTM(50))\n",
    "lstm_model.add(Dense(32, activation='relu'))\n",
    "lstm_model.add(Dense(\n",
    "    1, activation='sigmoid'))  # sigmoid since binary classification\n",
    "lstm_model.compile(optimizer='rmsprop',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['acc'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:52:36.092380Z",
     "start_time": "2021-09-28T23:49:44.986734Z"
    }
   },
   "outputs": [],
   "source": [
    "lstm_model.fit(X_train_padded, y_train, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T23:52:41.689702Z",
     "start_time": "2021-09-28T23:52:36.689350Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "y_pred = (lstm_model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment Analysis IMDb Reviews.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
